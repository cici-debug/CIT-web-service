Node:
(type) c
(layer)0
	Element:body
						Node:
						(type) h2
						(layer)2
							Element:h2
							Element:a
								Reading and Writing documents
							Element:a
								edit
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Introduction
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											Each index in Elasticsearch is
										Element:a
											divided into shards
											and each shard can have multiple copies. These copies are known as a
										Element:span
											and must be kept in sync when documents are added or removed. If we fail to do so, reading from one copy will result in very different results than reading from another. The process of keeping the shard copies in sync and serving reads from them is what we call the
										Element:em
											replication group
										Element:span
											.
										Element:em
											data replication model
									Node:
									(type) c
									(layer)3
										Element:p
											Elasticsearch’s data replication model is based on the
										Element:span
											and is described very well in the
										Element:em
											primary-backup model
										Element:a
											PacificA paper
											of Microsoft Research. That model is based on having a single copy from the replication group that acts as the primary shard. The other copies are called
										Element:span
											. The primary serves as the main entry point for all indexing operations. It is in charge of validating them and making sure they are correct. Once an index operation has been accepted by the primary, the primary is also responsible for replicating the operation to the other copies.
										Element:em
											replica shards
									Node:
									(type) c
									(layer)3
										Element:p
											This purpose of this section is to give a high level overview of the Elasticsearch replication model and discuss the implications it has for various interactions between write and read operations.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Basic write model
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											Every indexing operation in Elasticsearch is first resolved to a replication group using
										Element:a
											routing
											, typically based on the document ID. Once the replication group has been determined, the operation is forwarded internally to the current
										Element:span
											of the group. The primary shard is responsible for validating the operation and forwarding it to the other replicas. Since replicas can be offline, the primary is not required to replicate to all replicas. Instead, Elasticsearch maintains a list of shard copies that should receive the operation. This list is called the
										Element:em
											primary shard
										Element:span
											and is maintained by the master node. As the name implies, these are the set of "good" shard copies that are guaranteed to have processed all of the index and delete operations that have been acknowledged to the user. The primary is responsible for maintaining this invariant and thus has to replicate all operations to each copy in this set.
										Element:em
											in-sync copies
									Node:
									(type) c
									(layer)3
										Element:p
											The primary shard follows this basic flow:
									Node:
									(type) c
									(layer)3
										Element:ol
										Element:li
											Validate incoming operation and reject it if structurally invalid (Example: have an object field where a number is expected)
										Element:li
											Execute the operation locally i.e. indexing or deleting the relevant document. This will also validate the content of fields and reject if needed (Example: a keyword value is too long for indexing in Lucene).
										Element:li
											Forward the operation to each replica in the current in-sync copies set. If there are multiple replicas, this is done in parallel.
										Element:li
											Once all replicas have successfully performed the operation and responded to the primary, the primary acknowledges the successful completion of the request to the client.
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Failure handling
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														Many things can go wrong during indexing — disks can get corrupted, nodes can be disconnected from each other, or some configuration mistake could cause an operation to fail on a replica despite it being successful on the primary. These are infrequent but the primary has to respond to them.
												Node:
												(type) c
												(layer)4
													Element:p
														In the case that the primary itself fails, the node hosting the primary will send a message to the master about it. The indexing operation will wait (up to 1 minute, by
													Element:a
														default
														) for the master to promote one of the replicas to be a new primary. The operation will then be forwarded to the new primary for processing. Note that the master also monitors the health of the nodes and may decide to proactively demote a primary. This typically happens when the node holding the primary is isolated from the cluster by a networking issue. See
													Element:a
														here
														for more details.
												Node:
												(type) c
												(layer)4
													Element:p
														Once the operation has been successfully performed on the primary, the primary has to deal with potential failures when executing it on the replica shards. This may be caused by an actual failure on the replica or due to a network issue preventing the operation from reaching the replica (or preventing the replica from responding). All of these share the same end result: a replica which is part of the in-sync replica set misses an operation that is about to be acknowledged. In order to avoid violating the invariant, the primary sends a message to the master requesting that the problematic shard be removed from the in-sync replica set. Only once removal of the shard has been acknowledged by the master does the primary acknowledge the operation. Note that the master will also instruct another node to start building a new shard copy in order to restore the system to a healthy state.
												Node:
												(type) c
												(layer)4
													Element:p
													Element:a
														While forwarding an operation to the replicas, the primary will use the replicas to validate that it is still the active primary. If the primary has been isolated due to a network partition (or a long GC) it may continue to process incoming indexing operations before realising that it has been demoted. Operations that come from a stale primary will be rejected by the replicas. When the primary receives a response from the replica rejecting its request because it is no longer the primary then it will reach out to the master and will learn that it has been replaced. The operation is then routed to the new primary.
												Node:
												(type) c
												(layer)4
													Element:p
													Element:strong
														What happens if there are no replicas?
												Node:
												(type) c
												(layer)4
													Element:p
														This is a valid scenario that can happen due to index configuration or simply because all the replicas have failed. In that case the primary is processing operations without any external validation, which may seem problematic. On the other hand, the primary cannot fail other shards on its own but request the master to do so on its behalf. This means that the master knows that the primary is the only single good copy. We are therefore guaranteed that the master will not promote any other (out-of-date) shard copy to be a new primary and that any operation indexed into the primary will not be lost. Of course, since at that point we are running with only single copy of the data, physical hardware issues can cause data loss. See
													Element:a
														the section called “Wait For Active Shards
													Element:a
														edit
														” for some mitigation options.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Basic read model
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											Reads in Elasticsearch can be very lightweight lookups by ID or a heavy search request with complex aggregations that take non-trivial CPU power. One of the beauties of the primary-backup model is that it keeps all shard copies identical (with the exception of in-flight operations). As such, a single in-sync copy is sufficient to serve read requests.
									Node:
									(type) c
									(layer)3
										Element:p
											When a read request is received by a node, that node is responsible for forwarding it to the nodes that hold the relevant shards, collating the responses, and responding to the client. We call that node the
										Element:span
											for that request. The basic flow is as follows:
										Element:em
											coordinating node
									Node:
									(type) c
									(layer)3
										Element:ol
										Element:li
											Resolve the read requests to the relevant shards. Note that since most searches will be sent to one or more indices, they typically need to read from multiple shards, each representing a different subset of the data.
										Element:li
											Select an active copy of each relevant shard, from the shard replication group. This can be either the primary or a replica. By default, Elasticsearch will simply round robin between the shard copies.
										Element:li
											Send shard level read requests to the selected copies.
										Element:li
											Combine the results and respond. Note that in the case of get by ID look up, only one shard is relevant and this step can be skipped.
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Failure handling
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														When a shard fails to respond to a read request, the coordinating node will select another copy from the same replication group and send the shard level search request to that copy instead. Repetitive failures can result in no shard copies being available. In some cases, such as
													Element:code
														_search
														, Elasticsearch will prefer to respond fast, albeit with partial results, instead of waiting for the issue to be resolved (partial results are indicated in the
													Element:code
														_shards
														header of the response).
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											A few simple implications
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											Each of these basic flows determines how Elasticsearch behaves as a system for both reads and writes. Furthermore, since read and write requests can be executed concurrently, these two basic flows interact with each other. This has a few inherent implications:
									Node:
									(type) c
									(layer)3
										Element:dl
										Element:dt
										Element:span
											Efficient reads
										Element:dd
											Under normal operation each read operation is performed once for each relevant replication group. Only under failure conditions do multiple copies of the same shard execute the same search.
										Element:dt
										Element:span
											Read unacknowledged
										Element:dd
											Since the primary first indexes locally and then replicates the request, it is possible for a concurrent read to already see the change before it has been acknowledged.
										Element:dt
										Element:span
											Two copies by default
										Element:dd
											This model can be fault tolerant while maintaining only two copies of the data. This is in contrast to quorum-based system where the minimum number of copies for fault tolerance is 3.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Failures
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											Under failures, the following is possible:
									Node:
									(type) c
									(layer)3
										Element:dl
										Element:dt
										Element:span
											A single shard can slow down indexing
										Element:dd
											Because the primary waits for all replicas in the in-sync copies set during each operation, a single slow shard can slow down the entire replication group. This is the price we pay for the read efficiency mentioned above. Of course a single slow shard will also slow down unlucky searches that have been routed to it.
										Element:dt
										Element:span
											Dirty reads
										Element:dd
											An isolated primary can expose writes that will not be acknowledged. This is caused by the fact that an isolated primary will only realize that it is isolated once it sends requests to its replicas or when reaching out to the master. At that point the operation is already indexed into the primary and can be read by a concurrent read. Elasticsearch mitigates this risk by pinging the master every second (by default) and rejecting indexing operations if no master is known.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											The Tip of the Iceberg
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											This document provides a high level overview of how Elasticsearch deals with data. Of course, there is much much more going on under the hood. Things like primary terms, cluster state publishing and master election all play a role in keeping this system behaving correctly. This document also doesn’t cover known and important bugs (both closed and open). We recognize that
										Element:a
											GitHub is hard to keep up with
											. To help people stay on top of those and we maintain a dedicated
										Element:a
											resiliency page
											on our website. We strongly advise reading it.
									Node:
									(type) c
									(layer)3
										Element:span
										Element:a
											« Document APIs
									Node:
									(type) c
									(layer)3
										Element:span
										Element:a
											Index API »
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
											Top Videos
									Node:
									(type) c
									(layer)3
										Element:ul
										Element:li
										Element:a
											Elasticsearch Demo
										Element:li
										Element:a
											Kibana 101
										Element:li
										Element:a
											Logstash Primer
									Node:
									(type) c
									(layer)3
															Node:
															(type) c
															(layer)5
																Element:h5
																	Be in the know with the latest and greatest from Elastic.
															Node:
															(type) c
															(layer)5
																Element:p
																	Thanks for subscribing! We'll keep you updated with new releases.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:footer
										Element:h3
										Element:a
											Products >
										Element:ul
										Element:li
										Element:a
											Elasticsearch
										Element:li
										Element:a
											Kibana
										Element:li
										Element:a
											Beats
										Element:li
										Element:a
											Logstash
										Element:li
										Element:a
											X-Pack
										Element:li
										Element:a
											Elastic Cloud
										Element:li
										Element:a
											Security (formerly Shield)
										Element:li
										Element:a
											Alerting (via Watcher)
										Element:li
										Element:a
											Monitoring (formerly Marvel)
										Element:li
										Element:a
											Graph
										Element:li
										Element:a
											Reporting
										Element:li
										Element:a
											Machine Learning
										Element:li
										Element:a
											ES-Hadoop
										Element:h3
											Resources
										Element:ul
										Element:li
										Element:a
											Blog
										Element:li
										Element:a
											Cloud Status
										Element:li
										Element:a
											Community
										Element:li
										Element:a
											Customers & Use Cases
										Element:li
										Element:a
											Documentation
										Element:li
										Element:a
											Elastic{ON} Events
										Element:li
										Element:a
											Forums
										Element:li
										Element:a
											Meetups
										Element:li
										Element:a
											Subscriptions
										Element:li
										Element:a
											Support Portal
										Element:li
										Element:a
											Videos & Webinars
										Element:li
										Element:a
											Training
										Element:h3
										Element:a
											About >
										Element:ul
										Element:li
										Element:a
											Careers/Jobs
										Element:li
										Element:a
											Contact
										Element:li
										Element:a
											Leadership
										Element:li
										Element:a
											Partners
										Element:li
										Element:a
											Press
										Element:h3
											Language
										Element:ul
										Element:li
										Element:a
											English
										Element:li
										Element:a
											Français
										Element:li
										Element:a
											Deutsch
										Element:li
										Element:a
											日本語
										Element:li
										Element:a
											한국어
										Element:p
											FOLLOW US
										Element:ul
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:ul
										Element:li
										Element:a
											Trademarks
										Element:li
										Element:a
											Terms of Use
										Element:li
										Element:a
											Privacy
										Element:li
										Element:a
											Cookie Policy
										Element:li
										Element:a
											Brand
										Element:a
										Element:img
										Element:p
											© 2017. All Rights Reserved - Elasticsearch
										Element:p
											Elasticsearch is a trademark of Elasticsearch BV, registered in the U.S. and in other countries
										Element:p
											Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of the
										Element:a
											Apache Software Foundation
											in the United States and/or other countries.
