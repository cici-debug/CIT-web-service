Node:
(type) c
(layer)0
	Element:body
						Node:
						(type) h2
						(layer)2
							Element:h2
							Element:a
								Significant Terms Aggregation
							Element:a
								edit
						Node:
						(type) c
						(layer)2
							Element:p
								An aggregation that returns interesting or unusual occurrences of terms in a set.
						Node:
						(type) c
						(layer)2
							Element:img
								The significant_terms aggregation can be very heavy when run on large indices. Work is in progress to provide more lightweight sampling techniques. As a result, the API for this feature may change in non-backwards compatible ways
						Node:
						(type) c
						(layer)2
							Element:p
							Element:strong
								Example use cases:
						Node:
						(type) c
						(layer)2
							Element:ul
							Element:li
								Suggesting "H5N1" when users search for "bird flu" in text
							Element:li
								Identifying the merchant that is the "common point of compromise" from the transaction history of credit card owners reporting loss
							Element:li
								Suggesting keywords relating to stock symbol $ATI for an automated news classifier
							Element:li
								Spotting the fraudulent doctor who is diagnosing more than his fair share of whiplash injuries
							Element:li
								Spotting the tire manufacturer who has a disproportionate number of blow-outs
						Node:
						(type) c
						(layer)2
							Element:p
								In all these cases the terms being selected are not simply the most popular terms in a set. They are the terms that have undergone a significant change in popularity measured between a
							Element:span
								and
							Element:em
								foreground
							Element:span
								set. If the term "H5N1" only exists in 5 documents in a 10 million document index and yet is found in 4 of the 100 documents that make up a user’s search results that is significant and probably very relevant to their search. 5/10,000,000 vs 4/100 is a big swing in frequency.
							Element:em
								background
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Single-set analysis
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											In the simplest case, the
										Element:span
											set of interest is the search results matched by a query and the
										Element:em
											foreground
										Element:span
											set used for statistical comparisons is the index or indices from which the results were gathered.
										Element:em
											background
									Node:
									(type) c
									(layer)3
										Element:p
											Example:
									Node:
									(type) c
									(layer)3
										Element:pre
											{ "query" : { "terms" : {"force" : [ "British Transport Police" ]} }, "aggregations" : { "significantCrimeTypes" : { "significant_terms" : { "field" : "crime_type" } } } }
									Node:
									(type) c
									(layer)3
										Element:p
											Response:
									Node:
									(type) c
									(layer)3
										Element:pre
											{ ... "aggregations" : { "significantCrimeTypes" : { "doc_count": 47347, "buckets" : [ { "key": "Bicycle theft", "doc_count": 3640, "score": 0.371235374214817, "bg_count": 66799 } ... ] } } }
									Node:
									(type) c
									(layer)3
										Element:p
											When querying an index of all crimes from all police forces, what these results show is that the British Transport Police force stand out as a force dealing with a disproportionately large number of bicycle thefts. Ordinarily, bicycle thefts represent only 1% of crimes (66799/5064554) but for the British Transport Police, who handle crime on railways and stations, 7% of crimes (3640/47347) is a bike theft. This is a significant seven-fold increase in frequency and so this anomaly was highlighted as the top crime type.
									Node:
									(type) c
									(layer)3
										Element:p
											The problem with using a query to spot anomalies is it only gives us one subset to use for comparisons. To discover all the other police forces' anomalies we would have to repeat the query for each of the different forces.
									Node:
									(type) c
									(layer)3
										Element:p
											This can be a tedious way to look for unusual patterns in an index
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Multi-set analysis
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											A simpler way to perform analysis across multiple categories is to use a parent-level aggregation to segment the data ready for analysis.
									Node:
									(type) c
									(layer)3
										Element:p
											Example using a parent aggregation for segmentation:
									Node:
									(type) c
									(layer)3
										Element:pre
											{ "aggregations": { "forces": { "terms": {"field": "force"}, "aggregations": { "significantCrimeTypes": { "significant_terms": {"field": "crime_type"} } } } } }
									Node:
									(type) c
									(layer)3
										Element:p
											Response:
									Node:
									(type) c
									(layer)3
										Element:pre
											{ ... "aggregations": { "forces": { "buckets": [ { "key": "Metropolitan Police Service", "doc_count": 894038, "significantCrimeTypes": { "doc_count": 894038, "buckets": [ { "key": "Robbery", "doc_count": 27617, "score": 0.0599, "bg_count": 53182 }, ... ] } }, { "key": "British Transport Police", "doc_count": 47347, "significantCrimeTypes": { "doc_count": 47347, "buckets": [ { "key": "Bicycle theft", "doc_count": 3640, "score": 0.371, "bg_count": 66799 }, ... ] } } ] } }
									Node:
									(type) c
									(layer)3
										Element:p
											Now we have anomaly detection for each of the police forces using a single request.
									Node:
									(type) c
									(layer)3
										Element:p
											We can use other forms of top-level aggregations to segment our data, for example segmenting by geographic area to identify unusual hot-spots of a particular crime type:
									Node:
									(type) c
									(layer)3
										Element:pre
											{ "aggs": { "hotspots": { "geohash_grid" : { "field":"location", "precision":5, }, "aggs": { "significantCrimeTypes": { "significant_terms": {"field": "crime_type"} } } } } }
									Node:
									(type) c
									(layer)3
										Element:p
											This example uses the
										Element:code
											geohash_grid
											aggregation to create result buckets that represent geographic areas, and inside each bucket we can identify anomalous levels of a crime type in these tightly-focused areas e.g.
									Node:
									(type) c
									(layer)3
										Element:ul
										Element:li
											Airports exhibit unusual numbers of weapon confiscations
										Element:li
											Universities show uplifts of bicycle thefts
									Node:
									(type) c
									(layer)3
										Element:p
											At a higher geohash_grid zoom-level with larger coverage areas we would start to see where an entire police-force may be tackling an unusual volume of a particular crime type.
									Node:
									(type) c
									(layer)3
										Element:p
											Obviously a time-based top-level segmentation would help identify current trends for each point in time where a simple
										Element:code
											terms
											aggregation would typically show the very popular "constants" that persist across all time slots.
									Node:
									(type) c
									(layer)3
										Element:p
										Element:strong
											How are the scores calculated?
									Node:
									(type) c
									(layer)3
										Element:p
											The numbers returned for scores are primarily intended for ranking different suggestions sensibly rather than something easily understood by end users. The scores are derived from the doc frequencies in
										Element:span
											and
										Element:em
											foreground
										Element:span
											sets. In brief, a term is considered significant if there is a noticeable difference in the frequency in which a term appears in the subset and in the background. The way the terms are ranked can be configured, see "Parameters" section.
										Element:em
											background
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Use on free-text fields
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											The significant_terms aggregation can be used effectively on tokenized free-text fields to suggest:
									Node:
									(type) c
									(layer)3
										Element:ul
										Element:li
											keywords for refining end-user searches
										Element:li
											keywords for use in percolator queries
									Node:
									(type) c
									(layer)3
										Element:img
									Node:
									(type) c
									(layer)3
										Element:p
											Picking a free-text field as the subject of a significant terms analysis can be expensive! It will attempt to load every unique word into RAM. It is recommended to only use this on smaller indices.
									Node:
									(type) c
									(layer)3
										Element:p
										Element:strong
											Use the
										Element:span
											pattern
										Element:em
											"like this but not this"
									Node:
									(type) c
									(layer)3
										Element:p
											You can spot mis-categorized content by first searching a structured field e.g.
										Element:code
											category:adultMovie
											and use significant_terms on the free-text "movie_description" field. Take the suggested words (I’ll leave them to your imagination) and then search for all movies NOT marked as category:adultMovie but containing these keywords. You now have a ranked list of badly-categorized movies that you should reclassify or at least remove from the "familyFriendly" category.
									Node:
									(type) c
									(layer)3
										Element:p
											The significance score from each term can also provide a useful
										Element:code
											boost
											setting to sort matches. Using the
										Element:code
											minimum_should_match
											setting of the
										Element:code
											terms
											query with the keywords will help control the balance of precision/recall in the result set i.e a high setting would have a small number of relevant results packed full of keywords and a setting of "1" would produce a more exhaustive results set with all documents containing
										Element:span
											keyword.
										Element:em
											any
									Node:
									(type) c
									(layer)3
										Element:img
									Node:
									(type) c
									(layer)3
										Element:p
										Element:strong
											Show significant_terms in context.
											Free-text significant_terms are much more easily understood when viewed in context. Take the results of
										Element:code
											significant_terms
											suggestions from a free-text field and use them in a
										Element:code
											terms
											query on the same field with a
										Element:code
											highlight
											clause to present users with example snippets of documents. When the terms are presented unstemmed, highlighted, with the right case, in the right order and with some context, their significance/meaning is more readily apparent.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Custom background sets
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											Ordinarily, the foreground set of documents is "diffed" against a background set of all the documents in your index. However, sometimes it may prove useful to use a narrower background set as the basis for comparisons. For example, a query on documents relating to "Madrid" in an index with content from all over the world might reveal that "Spanish" was a significant term. This may be true but if you want some more focused terms you could use a
										Element:code
											background_filter
											on the term
										Element:span
											to establish a narrower set of documents as context. With this as a background "Spanish" would now be seen as commonplace and therefore not as significant as words like "capital" that relate more strongly with Madrid. Note that using a background filter will slow things down - each term’s background frequency must now be derived on-the-fly from filtering posting lists rather than reading the index’s pre-computed count for a term.
										Element:em
											spain
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Limitations
										Element:a
											edit
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Significant terms must be indexed values
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														Unlike the terms aggregation it is currently not possible to use script-generated terms for counting purposes. Because of the way the significant_terms aggregation must consider both
													Element:span
														and
													Element:em
														foreground
													Element:span
														frequencies it would be prohibitively expensive to use a script on the entire index to obtain background frequencies for comparisons. Also DocValues are not supported as sources of term data for similar reasons.
													Element:em
														background
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														No analysis of floating point fields
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														Floating point fields are currently not supported as the subject of significant_terms analysis. While integer or long fields can be used to represent concepts like bank account numbers or category numbers which can be interesting to track, floating point fields are usually used to represent quantities of something. As such, individual floating point terms are not useful for this form of frequency analysis.
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Use as a parent aggregation
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														If there is the equivalent of a
													Element:code
														match_all
														query or no query criteria providing a subset of the index the significant_terms aggregation should not be used as the top-most aggregation - in this scenario the
													Element:span
														set is exactly the same as the
													Element:em
														foreground
													Element:span
														set and so there is no difference in document frequencies to observe and from which to make sensible suggestions.
													Element:em
														background
												Node:
												(type) c
												(layer)4
													Element:p
														Another consideration is that the significant_terms aggregation produces many candidate results at shard level that are only later pruned on the reducing node once all statistics from all shards are merged. As a result, it can be inefficient and costly in terms of RAM to embed large child aggregations under a significant_terms aggregation that later discards many candidate terms. It is advisable in these cases to perform two searches - the first to provide a rationalized list of significant_terms and then add this shortlist of terms to a second query to go back and fetch the required child aggregations.
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Approximate counts
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														The counts of how many documents contain a term provided in results are based on summing the samples returned from each shard and as such may be:
												Node:
												(type) c
												(layer)4
													Element:ul
													Element:li
														low if certain shards did not provide figures for a given term in their top sample
													Element:li
														high when considering the background frequency as it may count occurrences found in deleted documents
												Node:
												(type) c
												(layer)4
													Element:p
														Like most design decisions, this is the basis of a trade-off in which we have chosen to provide fast performance at the cost of some (typically small) inaccuracies. However, the
													Element:code
														size
														and
													Element:code
														shard size
														settings covered in the next section provide tools to help control the accuracy levels.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Parameters
										Element:a
											edit
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														JLH score
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														The scores are derived from the doc frequencies in
													Element:span
														and
													Element:em
														foreground
													Element:span
														sets. The
													Element:em
														background
													Element:span
														change in popularity (foregroundPercent - backgroundPercent) would favor common terms whereas the
													Element:em
														absolute
													Element:span
														change in popularity (foregroundPercent/ backgroundPercent) would favor rare terms. Rare vs common is essentially a precision vs recall balance and so the absolute and relative changes are multiplied to provide a sweet spot between precision and recall.
													Element:em
														relative
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														mutual information
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														Mutual information as described in "Information Retrieval", Manning et al., Chapter 13.5.1 can be used as significance score by adding the parameter
												Node:
												(type) c
												(layer)4
													Element:pre
														"mutual_information": { "include_negatives": true }
												Node:
												(type) c
												(layer)4
													Element:p
														Mutual information does not differentiate between terms that are descriptive for the subset or for documents outside the subset. The significant terms therefore can contain terms that appear more or less frequent in the subset than outside the subset. To filter out the terms that appear less often in the subset than in documents outside the subset,
													Element:code
														include_negatives
														can be set to
													Element:code
														false
														.
												Node:
												(type) c
												(layer)4
													Element:p
														Per default, the assumption is that the documents in the bucket are also contained in the background. If instead you defined a custom background filter that represents a different set of documents that you want to compare to, set
												Node:
												(type) c
												(layer)4
													Element:pre
														"background_is_superset": false
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Chi square
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														Chi square as described in "Information Retrieval", Manning et al., Chapter 13.5.2 can be used as significance score by adding the parameter
												Node:
												(type) c
												(layer)4
													Element:pre
														"chi_square": { }
												Node:
												(type) c
												(layer)4
													Element:p
														Chi square behaves like mutual information and can be configured with the same parameters
													Element:code
														include_negatives
														and
													Element:code
														background_is_superset
														.
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														google normalized distance
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														Google normalized distance as described in "The Google Similarity Distance", Cilibrasi and Vitanyi, 2007 (
													Element:a
														http://arxiv.org/pdf/cs/0412098v3.pdf
														) can be used as significance score by adding the parameter
												Node:
												(type) c
												(layer)4
													Element:pre
														"gnd": { }
												Node:
												(type) c
												(layer)4
													Element:p
													Element:code
														gnd
														also accepts the
													Element:code
														background_is_superset
														parameter.
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Percentage
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														A simple calculation of the number of documents in the foreground sample with a term divided by the number of documents in the background with the term. By default this produces a score greater than zero and less than one.
												Node:
												(type) c
												(layer)4
													Element:p
														The benefit of this heuristic is that the scoring logic is simple to explain to anyone familiar with a "per capita" statistic. However, for fields with high cardinality there is a tendency for this heuristic to select the rarest terms such as typos that occur only once because they score 1/1 = 100%.
												Node:
												(type) c
												(layer)4
													Element:p
														It would be hard for a seasoned boxer to win a championship if the prize was awarded purely on the basis of percentage of fights won - by these rules a newcomer with only one fight under his belt would be impossible to beat. Multiple observations are typically required to reinforce a view so it is recommended in these cases to set both
													Element:code
														min_doc_count
														and
													Element:code
														shard_min_doc_count
														to a higher value such as 10 in order to filter out the low-frequency terms that otherwise take precedence.
												Node:
												(type) c
												(layer)4
													Element:pre
														"percentage": { }
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Which one is best?
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														Roughly,
													Element:code
														mutual_information
														prefers high frequent terms even if they occur also frequently in the background. For example, in an analysis of natural language text this might lead to selection of stop words.
													Element:code
														mutual_information
														is unlikely to select very rare terms like misspellings.
													Element:code
														gnd
														prefers terms with a high co-occurrence and avoids selection of stopwords. It might be better suited for synonym detection. However,
													Element:code
														gnd
														has a tendency to select very rare terms that are, for example, a result of misspelling.
													Element:code
														chi_square
														and
													Element:code
														jlh
														are somewhat in-between.
												Node:
												(type) c
												(layer)4
													Element:p
														It is hard to say which one of the different heuristics will be the best choice as it depends on what the significant terms are used for (see for example [Yang and Pedersen, "A Comparative Study on Feature Selection in Text Categorization", 1997](
													Element:a
														http://courses.ischool.berkeley.edu/i256/f06/papers/yang97comparative.pdf
														) for a study on using significant terms for feature selection for text classification).
												Node:
												(type) c
												(layer)4
													Element:p
														If none of the above measures suits your usecase than another option is to implement a custom significance measure:
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														scripted
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														Customized scores can be implemented via a script:
												Node:
												(type) c
												(layer)4
													Element:pre
														"script_heuristic": { "script": { "lang": "painless", "inline": "params._subset_freq/(params._superset_freq - params._subset_freq + 1)" } }
												Node:
												(type) c
												(layer)4
													Element:p
														Scripts can be inline (as in above example), indexed or stored on disk. For details on the options, see
													Element:a
														script documentation
														.
												Node:
												(type) c
												(layer)4
													Element:p
														Available parameters in the script are
												Node:
												(type) c
												(layer)4
													Element:table
													Element:colgroup
													Element:col
													Element:col
													Element:tbody
													Element:tr
													Element:td
													Element:p
													Element:code
														_subset_freq
													Element:td
													Element:p
														Number of documents the term appears in in the subset.
													Element:tr
													Element:td
													Element:p
													Element:code
														_superset_freq
													Element:td
													Element:p
														Number of documents the term appears in in the superset.
													Element:tr
													Element:td
													Element:p
													Element:code
														_subset_size
													Element:td
													Element:p
														Number of documents in the subset.
													Element:tr
													Element:td
													Element:p
													Element:code
														_superset_size
													Element:td
													Element:p
														Number of documents in the superset.
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Size & Shard Size
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														The
													Element:code
														size
														parameter can be set to define how many term buckets should be returned out of the overall terms list. By default, the node coordinating the search process will request each shard to provide its own top term buckets and once all shards respond, it will reduce the results to the final list that will then be returned to the client. If the number of unique terms is greater than
													Element:code
														size
														, the returned list can be slightly off and not accurate (it could be that the term counts are slightly off and it could even be that a term that should have been in the top size buckets was not returned).
												Node:
												(type) c
												(layer)4
													Element:p
														To ensure better accuracy a multiple of the final
													Element:code
														size
														is used as the number of terms to request from each shard using a heuristic based on the number of shards. To take manual control of this setting the
													Element:code
														shard_size
														parameter can be used to control the volumes of candidate terms produced by each shard.
												Node:
												(type) c
												(layer)4
													Element:p
														Low-frequency terms can turn out to be the most interesting ones once all results are combined so the significant_terms aggregation can produce higher-quality results when the
													Element:code
														shard_size
														parameter is set to values significantly higher than the
													Element:code
														size
														setting. This ensures that a bigger volume of promising candidate terms are given a consolidated review by the reducing node before the final selection. Obviously large candidate term lists will cause extra network traffic and RAM usage so this is quality/cost trade off that needs to be balanced. If
													Element:code
														shard_size
														is set to -1 (the default) then
													Element:code
														shard_size
														will be automatically estimated based on the number of shards and the
													Element:code
														size
														parameter.
												Node:
												(type) c
												(layer)4
													Element:img
												Node:
												(type) c
												(layer)4
													Element:p
													Element:code
														shard_size
														cannot be smaller than
													Element:code
														size
														(as it doesn’t make much sense). When it is, elasticsearch will override it and reset it to be equal to
													Element:code
														size
														.
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Minimum document count
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														It is possible to only return terms that match more than a configured number of hits using the
													Element:code
														min_doc_count
														option:
												Node:
												(type) c
												(layer)4
													Element:pre
														{ "aggs" : { "tags" : { "significant_terms" : { "field" : "tag", "min_doc_count": 10 } } } }
												Node:
												(type) c
												(layer)4
													Element:p
														The above aggregation would only return tags which have been found in 10 hits or more. Default value is
													Element:code
														3
														.
												Node:
												(type) c
												(layer)4
													Element:p
														Terms that score highly will be collected on a shard level and merged with the terms collected from other shards in a second step. However, the shard does not have the information about the global term frequencies available. The decision if a term is added to a candidate list depends only on the score computed on the shard using local shard frequencies, not the global frequencies of the word. The
													Element:code
														min_doc_count
														criterion is only applied after merging local terms statistics of all shards. In a way the decision to add the term as a candidate is made without being very
													Element:span
														about if the term will actually reach the required
													Element:em
														certain
													Element:code
														min_doc_count
														. This might cause many (globally) high frequent terms to be missing in the final result if low frequent but high scoring terms populated the candidate lists. To avoid this, the
													Element:code
														shard_size
														parameter can be increased to allow more candidate terms on the shards. However, this increases memory consumption and network traffic.
												Node:
												(type) c
												(layer)4
													Element:p
													Element:code
														shard_min_doc_count
														parameter
												Node:
												(type) c
												(layer)4
													Element:p
														The parameter
													Element:code
														shard_min_doc_count
														regulates the
													Element:span
														a shard has if the term should actually be added to the candidate list or not with respect to the
													Element:em
														certainty
													Element:code
														min_doc_count
														. Terms will only be considered if their local shard frequency within the set is higher than the
													Element:code
														shard_min_doc_count
														. If your dictionary contains many low frequent words and you are not interested in these (for example misspellings), then you can set the
													Element:code
														shard_min_doc_count
														parameter to filter out candidate terms on a shard level that will with a reasonable certainty not reach the required
													Element:code
														min_doc_count
														even after merging the local frequencies.
													Element:code
														shard_min_doc_count
														is set to
													Element:code
														1
														per default and has no effect unless you explicitly set it.
												Node:
												(type) c
												(layer)4
													Element:img
												Node:
												(type) c
												(layer)4
													Element:p
														Setting
													Element:code
														min_doc_count
														to
													Element:code
														1
														is generally not advised as it tends to return terms that are typos or other bizarre curiosities. Finding more than one instance of a term helps reinforce that, while still rare, the term was not the result of a one-off accident. The default value of 3 is used to provide a minimum weight-of-evidence. Setting
													Element:code
														shard_min_doc_count
														too high will cause significant candidate terms to be filtered out on a shard level. This value should be set much lower than
													Element:code
														min_doc_count/#shards
														.
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Custom background context
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														The default source of statistical information for background term frequencies is the entire index and this scope can be narrowed through the use of a
													Element:code
														background_filter
														to focus in on significant terms within a narrower context:
												Node:
												(type) c
												(layer)4
													Element:pre
														{ "query" : { "match" : "madrid" }, "aggs" : { "tags" : { "significant_terms" : { "field" : "tag", "background_filter": { "term" : { "text" : "spain"} } } } } }
												Node:
												(type) c
												(layer)4
													Element:p
														The above filter would help focus in on terms that were peculiar to the city of Madrid rather than revealing terms like "Spanish" that are unusual in the full index’s worldwide context but commonplace in the subset of documents containing the word "Spain".
												Node:
												(type) c
												(layer)4
													Element:img
												Node:
												(type) c
												(layer)4
													Element:p
														Use of background filters will slow the query as each term’s postings must be filtered to determine a frequency
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Filtering Values
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														It is possible (although rarely required) to filter the values for which buckets will be created. This can be done using the
													Element:code
														include
														and
													Element:code
														exclude
														parameters which are based on a regular expression string or arrays of exact terms. This functionality mirrors the features described in the
													Element:a
														terms aggregation
														documentation.
									Node:
									(type) c
									(layer)3
												Node:
												(type) c
												(layer)4
													Element:h4
													Element:a
														Execution hint
													Element:a
														edit
												Node:
												(type) c
												(layer)4
													Element:p
														There are different mechanisms by which terms aggregations can be executed:
												Node:
												(type) c
												(layer)4
													Element:ul
													Element:li
														by using field values directly in order to aggregate data per-bucket (
													Element:code
														map
														)
													Element:li
														by using ordinals of the field and preemptively allocating one bucket per ordinal value (
													Element:code
														global_ordinals
														)
													Element:li
														by using ordinals of the field and dynamically allocating one bucket per ordinal value (
													Element:code
														global_ordinals_hash
														)
												Node:
												(type) c
												(layer)4
													Element:p
														Elasticsearch tries to have sensible defaults so this is something that generally doesn’t need to be configured.
												Node:
												(type) c
												(layer)4
													Element:p
													Element:code
														map
														should only be considered when very few documents match a query. Otherwise the ordinals-based execution modes are significantly faster. By default,
													Element:code
														map
														is only used when running an aggregation on scripts, since they don’t have ordinals.
												Node:
												(type) c
												(layer)4
													Element:p
													Element:code
														global_ordinals
														is the second fastest option, but the fact that it preemptively allocates buckets can be memory-intensive, especially if you have one or more sub aggregations. It is used by default on top-level terms aggregations.
												Node:
												(type) c
												(layer)4
													Element:p
													Element:code
														global_ordinals_hash
														on the contrary to
													Element:code
														global_ordinals
														and
													Element:code
														global_ordinals_low_cardinality
														allocates buckets dynamically so memory usage is linear to the number of values of the documents that are part of the aggregation scope. It is used by default in inner aggregations.
												Node:
												(type) c
												(layer)4
													Element:pre
														{ "aggs" : { "tags" : { "significant_terms" : { "field" : "tags", "execution_hint": "map"
													Element:a
													Element:span
														} } } }
													Element:img
												Node:
												(type) c
												(layer)4
													Element:table
													Element:tr
													Element:td
													Element:p
													Element:a
													Element:span
													Element:img
													Element:td
													Element:p
														the possible values are
													Element:code
														map
														,
													Element:code
														global_ordinals
														and
													Element:code
														global_ordinals_hash
												Node:
												(type) c
												(layer)4
													Element:p
														Please note that Elasticsearch will ignore this execution hint if it is not applicable.
												Node:
												(type) c
												(layer)4
													Element:span
													Element:a
														« Sampler Aggregation
												Node:
												(type) c
												(layer)4
													Element:span
													Element:a
														Terms Aggregation »
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
											Top Videos
									Node:
									(type) c
									(layer)3
										Element:ul
										Element:li
										Element:a
											Elasticsearch Demo
										Element:li
										Element:a
											Kibana 101
										Element:li
										Element:a
											Logstash Primer
									Node:
									(type) c
									(layer)3
															Node:
															(type) c
															(layer)5
																Element:h5
																	Be in the know with the latest and greatest from Elastic.
															Node:
															(type) c
															(layer)5
																Element:p
																	Thanks for subscribing! We'll keep you updated with new releases.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:footer
										Element:h3
										Element:a
											Products >
										Element:ul
										Element:li
										Element:a
											Elasticsearch
										Element:li
										Element:a
											Kibana
										Element:li
										Element:a
											Beats
										Element:li
										Element:a
											Logstash
										Element:li
										Element:a
											X-Pack
										Element:li
										Element:a
											Elastic Cloud
										Element:li
										Element:a
											Security (formerly Shield)
										Element:li
										Element:a
											Alerting (via Watcher)
										Element:li
										Element:a
											Monitoring (formerly Marvel)
										Element:li
										Element:a
											Graph
										Element:li
										Element:a
											Reporting
										Element:li
										Element:a
											Machine Learning
										Element:li
										Element:a
											ES-Hadoop
										Element:h3
											Resources
										Element:ul
										Element:li
										Element:a
											Blog
										Element:li
										Element:a
											Cloud Status
										Element:li
										Element:a
											Community
										Element:li
										Element:a
											Customers & Use Cases
										Element:li
										Element:a
											Documentation
										Element:li
										Element:a
											Elastic{ON} Events
										Element:li
										Element:a
											Forums
										Element:li
										Element:a
											Meetups
										Element:li
										Element:a
											Subscriptions
										Element:li
										Element:a
											Support Portal
										Element:li
										Element:a
											Videos & Webinars
										Element:li
										Element:a
											Training
										Element:h3
										Element:a
											About >
										Element:ul
										Element:li
										Element:a
											Careers/Jobs
										Element:li
										Element:a
											Contact
										Element:li
										Element:a
											Leadership
										Element:li
										Element:a
											Partners
										Element:li
										Element:a
											Press
										Element:h3
											Language
										Element:ul
										Element:li
										Element:a
											English
										Element:li
										Element:a
											Français
										Element:li
										Element:a
											Deutsch
										Element:li
										Element:a
											日本語
										Element:li
										Element:a
											한국어
										Element:p
											FOLLOW US
										Element:ul
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:ul
										Element:li
										Element:a
											Trademarks
										Element:li
										Element:a
											Terms of Use
										Element:li
										Element:a
											Privacy
										Element:li
										Element:a
											Cookie Policy
										Element:li
										Element:a
											Brand
										Element:a
										Element:img
										Element:p
											© 2017. All Rights Reserved - Elasticsearch
										Element:p
											Elasticsearch is a trademark of Elasticsearch BV, registered in the U.S. and in other countries
										Element:p
											Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of the
										Element:a
											Apache Software Foundation
											in the United States and/or other countries.
