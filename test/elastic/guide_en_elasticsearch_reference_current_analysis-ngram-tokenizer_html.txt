Node:
(type) c
(layer)0
	Element:body
						Node:
						(type) h2
						(layer)2
							Element:h2
							Element:a
								NGram Tokenizer
							Element:a
								edit
						Node:
						(type) c
						(layer)2
							Element:p
								The
							Element:code
								ngram
								tokenizer first breaks text down into words whenever it encounters one of a list of specified characters, then it emits
							Element:a
								N-grams
								of each word of the specified length.
						Node:
						(type) c
						(layer)2
							Element:p
								N-grams are like a sliding window that moves across the word - a continuous sequence of characters of the specified length. They are useful for querying languages that don’t use spaces or that have long compound words, like German.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Example output
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											With the default settings, the
										Element:code
											ngram
											tokenizer treats the initial text as a single token and produces N-grams with minimum length
										Element:code
											1
											and maximum length
										Element:code
											2
											:
									Node:
									(type) c
									(layer)3
										Element:pre
											POST _analyze { "tokenizer": "ngram", "text": "Quick Fox" }
									Node:
									(type) c
									(layer)3
										Element:p
											The above sentence would produce the following terms:
									Node:
									(type) c
									(layer)3
										Element:pre
											[ Q, Qu, u, ui, i, ic, c, ck, k, "k ", " ", " F", F, Fo, o, ox, x ]
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Configuration
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											The
										Element:code
											ngram
											tokenizer accepts the following parameters:
									Node:
									(type) c
									(layer)3
										Element:table
										Element:colgroup
										Element:col
										Element:col
										Element:tbody
										Element:tr
										Element:td
										Element:p
										Element:code
											min_gram
										Element:td
										Element:p
											Minimum length of characters in a gram. Defaults to
										Element:code
											1
											.
										Element:tr
										Element:td
										Element:p
										Element:code
											max_gram
										Element:td
										Element:p
											Maximum length of characters in a gram. Defaults to
										Element:code
											2
											.
										Element:tr
										Element:td
										Element:p
										Element:code
											token_chars
										Element:td
										Element:p
											Character classes that should be included in a token. Elasticsearch will split on characters that don’t belong to the classes specified. Defaults to
										Element:code
											[]
											(keep all characters).
										Element:p
											Character classes may be any of the following:
										Element:ul
										Element:li
										Element:code
											letter
											— for example
										Element:code
											a
											,
										Element:code
											b
											,
										Element:code
											ï
											or
										Element:code
											京
										Element:li
										Element:code
											digit
											— for example
										Element:code
											3
											or
										Element:code
											7
										Element:li
										Element:code
											whitespace
											— for example
										Element:code
											" "
											or
										Element:code
											"\n"
										Element:li
										Element:code
											punctuation
											— for example
										Element:code
											!
											or
										Element:code
											"
										Element:li
										Element:code
											symbol
											— for example
										Element:code
											$
											or
										Element:code
											√
									Node:
									(type) c
									(layer)3
										Element:img
									Node:
									(type) c
									(layer)3
										Element:p
											It usually makes sense to set
										Element:code
											min_gram
											and
										Element:code
											max_gram
											to the same value. The smaller the length, the more documents will match but the lower the quality of the matches. The longer the length, the more specific the matches. A tri-gram (length
										Element:code
											3
											) is a good place to start.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Example configuration
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											In this example, we configure the
										Element:code
											ngram
											tokenizer to treat letters and digits as tokens, and to produce tri-grams (grams of length
										Element:code
											3
											):
									Node:
									(type) c
									(layer)3
										Element:pre
											PUT my_index { "settings": { "analysis": { "analyzer": { "my_analyzer": { "tokenizer": "my_tokenizer" } }, "tokenizer": { "my_tokenizer": { "type": "ngram", "min_gram": 3, "max_gram": 3, "token_chars": [ "letter", "digit" ] } } } } } POST my_index/_analyze { "analyzer": "my_analyzer", "text": "2 Quick Foxes." }
									Node:
									(type) c
									(layer)3
										Element:p
											The above example produces the following terms:
									Node:
									(type) c
									(layer)3
										Element:pre
											[ Qui, uic, ick, Fox, oxe, xes ]
									Node:
									(type) c
									(layer)3
										Element:span
										Element:a
											« Thai Tokenizer
									Node:
									(type) c
									(layer)3
										Element:span
										Element:a
											Edge NGram Tokenizer »
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
											Top Videos
									Node:
									(type) c
									(layer)3
										Element:ul
										Element:li
										Element:a
											Elasticsearch Demo
										Element:li
										Element:a
											Kibana 101
										Element:li
										Element:a
											Logstash Primer
									Node:
									(type) c
									(layer)3
															Node:
															(type) c
															(layer)5
																Element:h5
																	Be in the know with the latest and greatest from Elastic.
															Node:
															(type) c
															(layer)5
																Element:p
																	Thanks for subscribing! We'll keep you updated with new releases.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:footer
										Element:h3
										Element:a
											Products >
										Element:ul
										Element:li
										Element:a
											Elasticsearch
										Element:li
										Element:a
											Kibana
										Element:li
										Element:a
											Beats
										Element:li
										Element:a
											Logstash
										Element:li
										Element:a
											X-Pack
										Element:li
										Element:a
											Elastic Cloud
										Element:li
										Element:a
											Security (formerly Shield)
										Element:li
										Element:a
											Alerting (via Watcher)
										Element:li
										Element:a
											Monitoring (formerly Marvel)
										Element:li
										Element:a
											Graph
										Element:li
										Element:a
											Reporting
										Element:li
										Element:a
											Machine Learning
										Element:li
										Element:a
											ES-Hadoop
										Element:h3
											Resources
										Element:ul
										Element:li
										Element:a
											Blog
										Element:li
										Element:a
											Cloud Status
										Element:li
										Element:a
											Community
										Element:li
										Element:a
											Customers & Use Cases
										Element:li
										Element:a
											Documentation
										Element:li
										Element:a
											Elastic{ON} Events
										Element:li
										Element:a
											Forums
										Element:li
										Element:a
											Meetups
										Element:li
										Element:a
											Subscriptions
										Element:li
										Element:a
											Support Portal
										Element:li
										Element:a
											Videos & Webinars
										Element:li
										Element:a
											Training
										Element:h3
										Element:a
											About >
										Element:ul
										Element:li
										Element:a
											Careers/Jobs
										Element:li
										Element:a
											Contact
										Element:li
										Element:a
											Leadership
										Element:li
										Element:a
											Partners
										Element:li
										Element:a
											Press
										Element:h3
											Language
										Element:ul
										Element:li
										Element:a
											English
										Element:li
										Element:a
											Français
										Element:li
										Element:a
											Deutsch
										Element:li
										Element:a
											日本語
										Element:li
										Element:a
											한국어
										Element:p
											FOLLOW US
										Element:ul
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:ul
										Element:li
										Element:a
											Trademarks
										Element:li
										Element:a
											Terms of Use
										Element:li
										Element:a
											Privacy
										Element:li
										Element:a
											Cookie Policy
										Element:li
										Element:a
											Brand
										Element:a
										Element:img
										Element:p
											© 2017. All Rights Reserved - Elasticsearch
										Element:p
											Elasticsearch is a trademark of Elasticsearch BV, registered in the U.S. and in other countries
										Element:p
											Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of the
										Element:a
											Apache Software Foundation
											in the United States and/or other countries.
