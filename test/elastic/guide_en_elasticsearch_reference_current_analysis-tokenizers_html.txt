Node:
(type) c
(layer)0
	Element:body
						Node:
						(type) h2
						(layer)2
							Element:h2
							Element:a
								Tokenizers
							Element:a
								edit
						Node:
						(type) c
						(layer)2
							Element:p
								A
							Element:span
								receives a stream of characters, breaks it up into individual
							Element:em
								tokenizer
							Element:span
								(usually individual words), and outputs a stream of
							Element:em
								tokens
							Element:span
								. For instance, a
							Element:em
								tokens
							Element:a
								tokenizer breaks text into tokens whenever it sees any whitespace. It would convert the text
							Element:code
								whitespace
							Element:code
								"Quick brown fox!"
								into the terms
							Element:code
								[Quick, brown, fox!]
								.
						Node:
						(type) c
						(layer)2
							Element:p
								The tokenizer is also responsible for recording the order or
							Element:span
								of each term (used for phrase and word proximity queries) and the start and end
							Element:em
								position
							Element:span
								of the original word which the term represents (used for highlighting search snippets).
							Element:em
								character offsets
						Node:
						(type) c
						(layer)2
							Element:p
								Elasticsearch has a number of built in tokenizers which can be used to build
							Element:a
								custom analyzers
								.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Word Oriented Tokenizers
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											The following tokenizers are usually used for tokenizing full text into individual words:
									Node:
									(type) c
									(layer)3
										Element:dl
										Element:dt
										Element:span
										Element:a
											Standard Tokenizer
										Element:dd
											The
										Element:code
											standard
											tokenizer divides text into terms on word boundaries, as defined by the Unicode Text Segmentation algorithm. It removes most punctuation symbols. It is the best choice for most languages.
										Element:dt
										Element:span
										Element:a
											Letter Tokenizer
										Element:dd
											The
										Element:code
											letter
											tokenizer divides text into terms whenever it encounters a character which is not a letter.
										Element:dt
										Element:span
										Element:a
											Lowercase Tokenizer
										Element:dd
											The
										Element:code
											lowercase
											tokenizer, like the
										Element:code
											letter
											tokenizer, divides text into terms whenever it encounters a character which is not a letter, but it also lowercases all terms.
										Element:dt
										Element:span
										Element:a
											Whitespace Tokenizer
										Element:dd
											The
										Element:code
											whitespace
											tokenizer divides text into terms whenever it encounters any whitespace character.
										Element:dt
										Element:span
										Element:a
											UAX URL Email Tokenizer
										Element:dd
											The
										Element:code
											uax_url_email
											tokenizer is like the
										Element:code
											standard
											tokenizer except that it recognises URLs and email addresses as single tokens.
										Element:dt
										Element:span
										Element:a
											Classic Tokenizer
										Element:dd
											The
										Element:code
											classic
											tokenizer is a grammar based tokenizer for the English Language.
										Element:dt
										Element:span
										Element:a
											Thai Tokenizer
										Element:dd
											The
										Element:code
											thai
											tokenizer segments Thai text into words.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Partial Word Tokenizers
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											These tokenizers break up text or words into small fragments, for partial word matching:
									Node:
									(type) c
									(layer)3
										Element:dl
										Element:dt
										Element:span
										Element:a
											N-Gram Tokenizer
										Element:dd
											The
										Element:code
											ngram
											tokenizer can break up text into words when it encounters any of a list of specified characters (e.g. whitespace or punctuation), then it returns n-grams of each word: a sliding window of continuous letters, e.g.
										Element:code
											quick
											→
										Element:code
											[qu, ui, ic, ck]
											.
										Element:dt
										Element:span
										Element:a
											Edge N-Gram Tokenizer
										Element:dd
											The
										Element:code
											edge_ngram
											tokenizer can break up text into words when it encounters any of a list of specified characters (e.g. whitespace or punctuation), then it returns n-grams of each word which are anchored to the start of the word, e.g.
										Element:code
											quick
											→
										Element:code
											[q, qu, qui, quic, quick]
											.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
										Element:a
											Structured Text Tokenizers
										Element:a
											edit
									Node:
									(type) c
									(layer)3
										Element:p
											The following tokenizers are usually used with structured text like identifiers, email addresses, zip codes, and paths, rather than with full text:
									Node:
									(type) c
									(layer)3
										Element:dl
										Element:dt
										Element:span
										Element:a
											Keyword Tokenizer
										Element:dd
											The
										Element:code
											keyword
											tokenizer is a “noop” tokenizer that accepts whatever text it is given and outputs the exact same text as a single term. It can be combined with token filters like
										Element:a
											to normalise the analysed terms.
										Element:code
											lowercase
										Element:dt
										Element:span
										Element:a
											Pattern Tokenizer
										Element:dd
											The
										Element:code
											pattern
											tokenizer uses a regular expression to either split text into terms whenever it matches a word separator, or to capture matching text as terms.
										Element:dt
										Element:span
										Element:a
											Path Tokenizer
										Element:dd
											The
										Element:code
											path_hierarchy
											tokenizer takes a hierarchical value like a filesystem path, splits on the path separator, and emits a term for each component in the tree, e.g.
										Element:code
											/foo/bar/baz
											→
										Element:code
											[/foo, /foo/bar, /foo/bar/baz ]
											.
									Node:
									(type) c
									(layer)3
										Element:span
										Element:a
											« Normalizers
									Node:
									(type) c
									(layer)3
										Element:span
										Element:a
											Standard Tokenizer »
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:h3
											Top Videos
									Node:
									(type) c
									(layer)3
										Element:ul
										Element:li
										Element:a
											Elasticsearch Demo
										Element:li
										Element:a
											Kibana 101
										Element:li
										Element:a
											Logstash Primer
									Node:
									(type) c
									(layer)3
															Node:
															(type) c
															(layer)5
																Element:h5
																	Be in the know with the latest and greatest from Elastic.
															Node:
															(type) c
															(layer)5
																Element:p
																	Thanks for subscribing! We'll keep you updated with new releases.
						Node:
						(type) c
						(layer)2
									Node:
									(type) c
									(layer)3
										Element:footer
										Element:h3
										Element:a
											Products >
										Element:ul
										Element:li
										Element:a
											Elasticsearch
										Element:li
										Element:a
											Kibana
										Element:li
										Element:a
											Beats
										Element:li
										Element:a
											Logstash
										Element:li
										Element:a
											X-Pack
										Element:li
										Element:a
											Elastic Cloud
										Element:li
										Element:a
											Security (formerly Shield)
										Element:li
										Element:a
											Alerting (via Watcher)
										Element:li
										Element:a
											Monitoring (formerly Marvel)
										Element:li
										Element:a
											Graph
										Element:li
										Element:a
											Reporting
										Element:li
										Element:a
											Machine Learning
										Element:li
										Element:a
											ES-Hadoop
										Element:h3
											Resources
										Element:ul
										Element:li
										Element:a
											Blog
										Element:li
										Element:a
											Cloud Status
										Element:li
										Element:a
											Community
										Element:li
										Element:a
											Customers & Use Cases
										Element:li
										Element:a
											Documentation
										Element:li
										Element:a
											Elastic{ON} Events
										Element:li
										Element:a
											Forums
										Element:li
										Element:a
											Meetups
										Element:li
										Element:a
											Subscriptions
										Element:li
										Element:a
											Support Portal
										Element:li
										Element:a
											Videos & Webinars
										Element:li
										Element:a
											Training
										Element:h3
										Element:a
											About >
										Element:ul
										Element:li
										Element:a
											Careers/Jobs
										Element:li
										Element:a
											Contact
										Element:li
										Element:a
											Leadership
										Element:li
										Element:a
											Partners
										Element:li
										Element:a
											Press
										Element:h3
											Language
										Element:ul
										Element:li
										Element:a
											English
										Element:li
										Element:a
											Français
										Element:li
										Element:a
											Deutsch
										Element:li
										Element:a
											日本語
										Element:li
										Element:a
											한국어
										Element:p
											FOLLOW US
										Element:ul
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:li
										Element:a
										Element:ul
										Element:li
										Element:a
											Trademarks
										Element:li
										Element:a
											Terms of Use
										Element:li
										Element:a
											Privacy
										Element:li
										Element:a
											Cookie Policy
										Element:li
										Element:a
											Brand
										Element:a
										Element:img
										Element:p
											© 2017. All Rights Reserved - Elasticsearch
										Element:p
											Elasticsearch is a trademark of Elasticsearch BV, registered in the U.S. and in other countries
										Element:p
											Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of the
										Element:a
											Apache Software Foundation
											in the United States and/or other countries.
